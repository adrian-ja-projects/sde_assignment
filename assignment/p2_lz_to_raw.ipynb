{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notebook to read the raw assignment file and transform into the normalized tables to be uploaded into the cassandra db\n",
    "#Author: Adrian Jimenez 2022-05\n",
    "#----\n",
    "#To-do: add dynamic creation of target path for multiple loads\n",
    "#To-do: encapsulate the spark session for a potential caller to create and close the session\n",
    "import os\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "from delta import * #to-do remove the wild import from delta\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.types import DateType, StructType, StructField, StringType, TimestampType\n",
    "\n",
    "from etl_utils.table_schema import TableSchema\n",
    "from etl_utils.general_utils import create_table_in_metastore, persist_table, upsert_into_table\n",
    "\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get or create spark delta session\n",
    "builder = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .master(\"local\")\n",
    "    .appName(\"p2_lz_to_raw\")\n",
    ")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run variables\n",
    "#to-do this should be variables fetched by the caller or stored in a configuration table in sql \n",
    "table_name = 'r_session_events'\n",
    "schema_root_path = '/home/jovyan/work/data_lake/raw/schemas'\n",
    "player_session_schema = TableSchema(table_name, schema_root_path).load_schema_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read using saved schema and unzip raw file\n",
    "df_assignment_data = (\n",
    "    spark\n",
    "    .read\n",
    "    .format('json')\n",
    "    .schema(player_session_schema)\n",
    "    .load('/home/jovyan/work/data_lake/landingzone/assignment_data/assignment_data.jsonl.bz2')\n",
    ")\n",
    "\n",
    "#add data lake ts for insert and update\n",
    "FinlandTimeZone = pytz.timezone('Europe/Helsinki')##Assuming the correct zone is helsinki, otherwise it can be adjust to UTC as standard\n",
    "df_assignment_data = (\n",
    "    df_assignment_data\n",
    "    .select(\n",
    "        F.col(\"country\"),\n",
    "        F.col(\"event\"),\n",
    "        F.col(\"player_id\"),\n",
    "        F.col(\"session_id\"),\n",
    "        F.col(\"ts\"),\n",
    "        F.lit(datetime.now(FinlandTimeZone).strftime('%Y-%m-%d %H:%M:%S')).cast(TimestampType()).alias(\"DL_INSERT_TS\"),\n",
    "        F.lit(datetime.now(FinlandTimeZone).strftime('%Y-%m-%d %H:%M:%S')).cast(TimestampType()).alias(\"DL_UPDATE_TS\"),\n",
    "        F.col(\"ts\").cast(DateType()).alias(\"EVENT_DATE\"),\n",
    "        F.concat_ws(\"-\", F.col(\"event\"), F.col(\"session_id\")).alias(\"UNIQUE_ID_ROW\")\n",
    "    )\n",
    ")\n",
    "print(\"INFO: assignment data loaded in dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#work around to mimic a stream of events\n",
    "inputDF = df_assignment_data\n",
    "partition_key = \"EVENT_DATE\"\n",
    "dl_insert_ts_column = \"DL_INSERT_TS\"\n",
    "unique_id = \"UNIQUE_ID_ROW\"\n",
    "upsert_mode = \"append\"\n",
    "schema = 'assignment_data'\n",
    "table_name = 'r_session_events'\n",
    "dl_raw_path = f'/home/jovyan/work/data_lake/raw/{schema}/{table_name}/'\n",
    "#cache for faster insert\n",
    "inputDF.cache()    \n",
    "list_events_dates = inputDF.select(F.col(\"EVENT_DATE\").cast(StringType())).distinct().sort(F.col(\"EVENT_DATE\").asc()).rdd.flatMap(lambda x: x).collect()\n",
    "#for development purposes\n",
    "if debug:\n",
    "    list_events_dates = list_events_dates[4:7]\n",
    "    \n",
    "for d in list_events_dates:\n",
    "    #adding unique id to enable smaller batches\n",
    "    df_to_write = (inputDF.where(F.col(f\"{partition_key}\")==d).sort(F.col(\"ts\")))\n",
    "    if not os.path.exists(dl_raw_path):\n",
    "        persist_table(spark, df_to_write, dl_raw_path, partition_key, table_name)\n",
    "    else:\n",
    "        #if exists follow the upsert pattern\n",
    "        #to-do find the correct column for the upsert\n",
    "        upsert_into_table(spark, df_to_write, table_name, schema, dl_raw_path, unique_id,  dl_insert_ts_column, upsert_mode)\n",
    "inputDF.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
