{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1e1d38e-3aad-463b-8829-0c3899a822f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#author: Adrian J\n",
    "#TO-DO: move the foreachbatch functions to a util\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "from delta import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import TimestampType, DateType, IntegerType, StringType\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c54a10-2134-49d2-ac09-71ee492232b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get or create spark delta session\n",
    "builder = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .master(\"local\")\n",
    "    .appName(\"p0_stream_triggers\")\n",
    ")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c24d9028-fbec-4c9c-82da-d60df6b0ed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source table in raw zone\n",
    "source_schema = 'assignment_data'\n",
    "source_table_name = 'r_session_events'\n",
    "source_dl_raw_path = f'/home/jovyan/work/data_lake/raw/{source_schema}/{source_table_name}/'\n",
    "\n",
    "#delta sink table uc zone\n",
    "sink_dl_schema = 'uc_assignment'\n",
    "sink_table_name = 'uc_delta_session_events'\n",
    "sink_dl_uc_path = f'/home/jovyan/work/data_lake/use_case/{sink_dl_schema}/{sink_table_name}'\n",
    "\n",
    "#assuming finland time is the default time for the data lake ts\n",
    "FinlandTimeZone = pytz.timezone('Europe/Helsinki')\n",
    "#all columns from source\n",
    "source_select_columns = [\"country\", \"player_id\", \"session_id\", \"ts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "def80305-17f7-4dbe-a1c0-c8dff627ca64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: metadata = Row(version=1, timestamp=datetime.datetime(2022, 5, 4, 14, 45, 5, 897000), userId=None, userName=None, operation='MERGE', operationParameters={'matchedPredicates': '[{\"actionType\":\"update\"}]', 'predicate': '(t.session_id = i.session_id)', 'notMatchedPredicates': '[{\"actionType\":\"insert\"}]'}, job=None, notebook=None, clusterId=None, readVersion=0, isolationLevel='Serializable', isBlindAppend=False, operationMetrics={'numOutputRows': '17227', 'numTargetRowsInserted': '17227', 'numTargetRowsUpdated': '0', 'numTargetFilesAdded': '1', 'numTargetFilesRemoved': '0', 'numTargetRowsDeleted': '0', 'scanTimeMs': '2227', 'numSourceRows': '0', 'executionTimeMs': '132635', 'numTargetRowsCopied': '0', 'rewriteTimeMs': '130246'}, userMetadata=None, engineInfo='Apache-Spark/3.2.1 Delta-Lake/1.2.0')\n"
     ]
    }
   ],
   "source": [
    "#READ and WRITE START and END events to data lake as UPSERT\n",
    "#To-do move the method to outside this notebook\n",
    "def upsert_to_delta(microbatch_input, batchId):\n",
    "    \"Method used for upsert start events into event delta table\"\n",
    "    sink_start_dl_schema = 'uc_assignment'\n",
    "    sink_start_table_name = 'uc_delta_session_events'\n",
    "    sink_start_dl_uc_path = f'/home/jovyan/work/data_lake/use_case/{sink_start_dl_schema}/{sink_start_table_name}'\n",
    "    existingTable = DeltaTable.forPath(spark, sink_start_dl_uc_path)\n",
    "    #merge statement \n",
    "    existingTable.alias(\"t\") \\\n",
    "    .merge(\n",
    "        microbatch_input.alias(\"i\"),\n",
    "        \"t.session_id = i.session_id\") \\\n",
    "     .whenMatchedUpdate(\n",
    "        set = {\"end_ts\": F.col(\"i.end_ts\"), \"DL_UPDATE_TS\": F.col(\"i.DL_UPDATE_TS\"), \n",
    "               \"session_status\":F.col(\"i.session_status\"), \"EVENT_DATE\":F.col(\"i.EVENT_DATE\")}) \\\n",
    "     .whenNotMatchedInsert(\n",
    "        values = {\"country\": \"i.country\", \"player_id\":\"i.player_id\", \"session_id\": \"i.session_id\",\n",
    "                  \"start_ts\": \"i.start_ts\", \"end_ts\": \"i.end_ts\", \"session_status\":\"i.session_status\", \n",
    "                  \"DL_INSERT_TS\": \"i.DL_INSERT_TS\", \"DL_UPDATE_TS\":\"i.DL_UPDATE_TS\", \"EVENT_DATE\":\"i.EVENT_DATE\"} ) \\\n",
    "     .execute()\n",
    "    metadata = (existingTable.history(1))\n",
    "    #print(f'INFO: Upsert Mode -> {upsert_mode}')\n",
    "    print(f'INFO: Upsert for table r_session_events completed')\n",
    "    print(f\"INFO: metadata = {metadata.collect()[0]}\")\n",
    "\n",
    "#order\n",
    "order_col = [\"country\", \"player_id\", \"session_id\", \"start_ts\", \"end_ts\", \"session_status\", \"DL_INSERT_TS\", \"DL_UPDATE_TS\", \"EVENT_DATE\"]\n",
    "#read and transform as required\n",
    "strm_start_events = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"delta\")\n",
    "    .load(source_dl_raw_path)\n",
    "    .select(F.col(\"country\").alias(\"country\"),\n",
    "            F.col(\"player_id\").alias(\"player_id\"),\n",
    "            F.col(\"session_id\").cast(StringType()).alias(\"session_id\"),\n",
    "            F.when(F.col(\"event\")==\"start\", F.col(\"ts\").cast(TimestampType())).otherwise(F.lit(None)).alias(\"start_ts\"),\n",
    "            F.when(F.col(\"event\")==\"end\", F.col(\"ts\").cast(TimestampType())).otherwise(F.lit(None)).alias(\"end_ts\"),\n",
    "            F.when(F.col(\"event\")==\"start\", F.lit(\"0\").cast(IntegerType())).otherwise(F.lit(\"1\").cast(IntegerType())).alias(\"session_status\"),\n",
    "            F.lit(datetime.now(FinlandTimeZone).strftime('%Y-%m-%d %H:%M:%S')).cast(TimestampType()).alias(\"DL_INSERT_TS\"),\n",
    "            F.lit(datetime.now(FinlandTimeZone).strftime('%Y-%m-%d %H:%M:%S')).cast(TimestampType()).alias(\"DL_UPDATE_TS\"),\n",
    "            F.col(\"ts\").cast(DateType()).alias(\"EVENT_DATE\")))\n",
    "#group to merge the sessions from same day\n",
    "strm_start_events = (\n",
    "    strm_start_events\n",
    "    .groupBy(\"player_id\", \"session_id\")\n",
    "    .agg(F.max(\"country\").alias(\"country\"),\n",
    "         F.min(\"start_ts\").alias(\"start_ts\"), \n",
    "         F.max(\"end_ts\").alias(\"end_ts\"), \n",
    "         F.max(\"session_status\").alias(\"session_status\"),\n",
    "         F.min(\"DL_INSERT_TS\").alias(\"DL_INSERT_TS\"),\n",
    "         F.max(\"DL_UPDATE_TS\").alias(\"DL_UPDATE_TS\"),\n",
    "         F.max(\"EVENT_DATE\").alias(\"EVENT_DATE\"))##remove from table redundant\n",
    "    .select(*order_col))\n",
    "#upser for eachbacth \n",
    "strm_start_events = (\n",
    "    strm_start_events\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .foreachBatch(upsert_to_delta)\n",
    "    .outputMode(\"update\")\n",
    "    .option(\"checkpointLocation\", f\"{sink_dl_uc_path}/_checkpoint\")\n",
    "    .start(sink_dl_uc_path)\n",
    ")\n",
    "print(\"INFO: listener to write into delta initiated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
